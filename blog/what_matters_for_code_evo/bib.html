<ol id="refs-list">
<li id="ref-novikov2025alphaevolve" data-title="AlphaEvolve: A coding agent for scientific and algorithmic discovery" data-short-authors="A. Novikov et al." data-venue="" data-year="2025">Novikov, Alexander; Vu, Ngan; Eisenberger, Marvin; Dupont, Emilien; Huang, Po-Sen; Wagner, Adam Zsolt; Shirobokov, Sergey; Kozlovskii, Borislav; Ruiz, Francisco JR; Mehrabian, Abbas; et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131.</li>
<li id="ref-romera2024mathematical" data-title="Mathematical discoveries from program search with large language models" data-short-authors="B. Romera-Paredes et al." data-venue="Nature" data-year="2024">Romera-Paredes, Bernardino; Barekatain, Mohammadamin; Novikov, Alexander; Balog, Matej; Kumar, M Pawan; Dupont, Emilien; Ruiz, Francisco JR; Ellenberg, Jordan S; Wang, Pengming; Fawzi, Omar; et al. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468--475.</li>
<li id="ref-li2022competition" data-title="Competition-level code generation with alphacode" data-short-authors="Y. Li et al." data-venue="Science" data-year="2022">Li, Yujia; Choi, David; Chung, Junyoung; Kushman, Nate; Schrittwieser, Julian; Leblond, Remi; Eccles, Tom; Keeling, James; Gimeno, Felix; Dal Lago, Agustin; et al. (2022). Competition-level code generation with alphacode. Science, 378, 1092--1097.</li>
<li id="ref-sennrich2015neural" data-title="Neural machine translation of rare words with subword units" data-short-authors="R. Sennrich et al." data-venue="" data-year="2015">Sennrich, Rico; Haddow, Barry; Birch, Alexandra. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.</li>
<li id="ref-openevolve" data-title="OpenEvolve: an open-source evolutionary coding agent" data-short-authors="A. Sharma" data-venue="GitHub" data-year="2025" data-url="https://github.com/codelion/openevolve">Sharma, Asankhaya. (2025). OpenEvolve: an open-source evolutionary coding agent. GitHub.</li>
<li id="ref-bahdanau2016neuralmachinetranslationjointly" data-title="Neural Machine Translation by Jointly Learning to Align and Translate" data-short-authors="D. Bahdanau et al." data-venue="" data-year="2016" data-url="https://arxiv.org/abs/1409.0473">Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua. (2016). Neural Machine Translation by Jointly Learning to Align and Translate.</li>
<li id="ref-chen2021evaluating" data-title="Evaluating large language models trained on code" data-short-authors="M. Chen et al." data-venue="" data-year="2021">Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde De Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; et al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.</li>
<li id="ref-gao2012implementing" data-title="Implementing the Nelder-Mead simplex algorithm with adaptive parameters" data-short-authors="F. Gao &amp; L. Han" data-venue="Computational Optimization and Applications" data-year="2012">Gao, Fuchang; Han, Lixing. (2012). Implementing the Nelder-Mead simplex algorithm with adaptive parameters. Computational Optimization and Applications, 51, 259--277.</li>
<li id="ref-haugland2016minimum" data-title="The minimum overlap problem revisited" data-short-authors="J. K. Haugland" data-venue="" data-year="2016">Haugland, Jan Kristian. (2016). The minimum overlap problem revisited. arXiv preprint arXiv:1609.08000.</li>
<li id="ref-gonccalves2017hermite" data-title="Hermite polynomials, linear flows on the torus, and an uncertainty principle for roots" data-short-authors="F. Goncalves et al." data-venue="Journal of Mathematical Analysis and Applications" data-year="2017">Goncalves, Felipe; e Silva, Diogo Oliveira; Steinerberger, Stefan. (2017). Hermite polynomials, linear flows on the torus, and an uncertainty principle for roots. Journal of Mathematical Analysis and Applications, 451, 678--711.</li>
<li id="ref-lange2025shinkaevolve" data-title="Shinkaevolve: Towards open-ended and sample-efficient program evolution" data-short-authors="R. T. Lange et al." data-venue="" data-year="2025">Lange, Robert Tjarko; Imajuku, Yuki; Cetin, Edoardo. (2025). Shinkaevolve: Towards open-ended and sample-efficient program evolution. arXiv preprint arXiv:2509.19349.</li>
<li id="ref-sutton2019bitter" data-title="The bitter lesson" data-short-authors="R. Sutton" data-venue="Incomplete Ideas (blog)" data-year="2019">Sutton, Richard. (2019). The bitter lesson. Incomplete Ideas (blog), 13, 38.</li>
<li id="ref-tao2007good" data-title="What is good mathematics?" data-short-authors="T. Tao" data-venue="Bulletin of the American Mathematical Society" data-year="2007">Tao, Terence. (2007). What is good mathematics?. Bulletin of the American Mathematical Society, 44, 623--634.</li>
<li id="ref-akiba_shinkaevolve_icfp_2025" data-title="ShinkaEvolve in Action: How a Human--AI Partnership Conquered a Coding Challenge" data-short-authors="T. Akiba" data-venue="" data-year="2025" data-url="\url{https://sakana.ai/icfp-2025/}">Akiba, Takuya. (2025). ShinkaEvolve in Action: How a Human--AI Partnership Conquered a Coding Challenge.</li>
<li id="ref-mitchener2025kosmos" data-title="Kosmos: An AI Scientist for Autonomous Discovery" data-short-authors="L. Mitchener et al." data-venue="" data-year="2025">Mitchener, Ludovico; Yiu, Angela; Chang, Benjamin; Bourdenx, Mathieu; Nadolski, Tyler; Sulovari, Arvis; Landsness, Eric C; Barabasi, Daniel L; Narayanan, Siddharth; Evans, Nicky; et al. (2025). Kosmos: An AI Scientist for Autonomous Discovery. arXiv preprint arXiv:2511.02824.</li>
<li id="ref-lu2024ai" data-title="The ai scientist: Towards fully automated open-ended scientific discovery" data-short-authors="C. Lu et al." data-venue="" data-year="2024">Lu, Chris; Lu, Cong; Lange, Robert Tjarko; Foerster, Jakob; Clune, Jeff; Ha, David. (2024). The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292.</li>
<li id="ref-gottweis2025towards" data-title="Towards an AI co-scientist" data-short-authors="J. Gottweis et al." data-venue="" data-year="2025">Gottweis, Juraj; Weng, Wei-Hung; Daryin, Alexander; Tu, Tao; Palepu, Anil; Sirkovic, Petar; Myaskovsky, Artiom; Weissenberger, Felix; Rong, Keran; Tanno, Ryutaro; et al. (2025). Towards an AI co-scientist. arXiv preprint arXiv:2502.18864.</li>
<li id="ref-langdon1997fitness" data-title="Fitness causes bloat" data-short-authors="W. B. Langdon &amp; R. Poli" data-venue="Soft Computing in Engineering Design and Manufacturing" data-year="1997">Langdon, William B; Poli, Riccardo. (1997). Fitness causes bloat. Soft Computing in Engineering Design and Manufacturing, 13--22.</li>
<li id="ref-hu2024automated" data-title="Automated design of agentic systems" data-short-authors="S. Hu et al." data-venue="" data-year="2024">Hu, Shengran; Lu, Cong; Clune, Jeff. (2024). Automated design of agentic systems. arXiv preprint arXiv:2408.08435.</li>
<li id="ref-chan2024mle" data-title="Mle-bench: Evaluating machine learning agents on machine learning engineering" data-short-authors="J. S. Chan et al." data-venue="" data-year="2024">Chan, Jun Shern; Chowdhury, Neil; Jaffe, Oliver; Aung, James; Sherburn, Dane; Mays, Evan; Starace, Giulio; Liu, Kevin; Maksin, Leon; Patwardhan, Tejal; et al. (2024). Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095.</li>
<li id="ref-georgiev2025mathematical" data-title="Mathematical exploration and discovery at scale" data-short-authors="B. Georgiev et al." data-venue="" data-year="2025">Georgiev, Bogdan; Gomez-Serrano, Javier; Tao, Terence; Wagner, Adam Zsolt. (2025). Mathematical exploration and discovery at scale. arXiv preprint arXiv:2511.02864.</li>
<li id="ref-el2025inefficiencies" data-title="Inefficiencies of Meta Agents for Agent Design" data-short-authors="B. El et al." data-venue="" data-year="2025">El, Batu; Yuksekgonul, Mert; Zou, James. (2025). Inefficiencies of Meta Agents for Agent Design. arXiv preprint arXiv:2510.06711.</li>
<li id="ref-greenblatt2024getting" data-title="Getting 50%(sota) on arc-agi with gpt-4o" data-short-authors="R. Greenblatt" data-venue="Redwood Research Blog, June" data-year="2024">Greenblatt, Ryan. (2024). Getting 50%(sota) on arc-agi with gpt-4o. Redwood Research Blog, June.</li>
<li id="ref-jordan2023variance" data-title="On the variance of neural network training with respect to test sets and distributions" data-short-authors="K. Jordan" data-venue="" data-year="2023">Jordan, Keller. (2023). On the variance of neural network training with respect to test sets and distributions. arXiv preprint arXiv:2304.01910.</li>
<li id="ref-agarwal2021deep" data-title="Deep reinforcement learning at the edge of the statistical precipice" data-short-authors="R. Agarwal et al." data-venue="Advances in neural information processing systems" data-year="2021">Agarwal, Rishabh; Schwarzer, Max; Castro, Pablo Samuel; Courville, Aaron C; Bellemare, Marc. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304--29320.</li>
<li id="ref-jiang2025aide" data-title="Aide: Ai-driven exploration in the space of code" data-short-authors="Z. Jiang et al." data-venue="" data-year="2025">Jiang, Zhengyao; Schmidt, Dominik; Srikanth, Dhruv; Xu, Dixing; Kaplan, Ian; Jacenko, Deniss; Wu, Yuxiang. (2025). Aide: Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138.</li>
<li id="ref-huang2023mlagentbench" data-title="Mlagentbench: Evaluating language agents on machine learning experimentation" data-short-authors="Q. Huang et al." data-venue="" data-year="2023">Huang, Qian; Vora, Jian; Liang, Percy; Leskovec, Jure. (2023). Mlagentbench: Evaluating language agents on machine learning experimentation. arXiv preprint arXiv:2310.03302.</li>
<li id="ref-wang2024openhands" data-title="Openhands: An open platform for ai software developers as generalist agents" data-short-authors="X. Wang et al." data-venue="" data-year="2024">Wang, Xingyao; Li, Boxuan; Song, Yufan; Xu, Frank F; Tang, Xiangru; Zhuge, Mingchen; Pan, Jiayi; Song, Yueqi; Li, Bowen; Singh, Jaskirat; et al. (2024). Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741.</li>
<li id="ref-schmidgall2025agent" data-title="Agent laboratory: Using llm agents as research assistants" data-short-authors="S. Schmidgall et al." data-venue="" data-year="2025">Schmidgall, Samuel; Su, Yusheng; Wang, Ze; Sun, Ximeng; Wu, Jialian; Yu, Xiaodong; Liu, Jiang; Liu, Zicheng; Barsoum, Emad. (2025). Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227.</li>
<li id="ref-toledo2025ai" data-title="AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench" data-short-authors="E. Toledo et al." data-venue="" data-year="2025">Toledo, Edan; Hambardzumyan, Karen; Josifoski, Martin; Hazra, Rishi; Baldwin, Nicolas; Audran-Reiss, Alexis; Kuchnik, Michael; Magka, Despoina; Jiang, Minqi; Lupidi, Alisia Maria; et al. (2025). AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench. arXiv preprint arXiv:2507.02554.</li>
<li id="ref-ma2023eureka" data-title="Eureka: Human-level reward design via coding large language models" data-short-authors="Y. J. Ma et al." data-venue="" data-year="2023">Ma, Yecheng Jason; Liang, William; Wang, Guanzhi; Huang, De-An; Bastani, Osbert; Jayaraman, Dinesh; Zhu, Yuke; Fan, Linxi; Anandkumar, Anima. (2023). Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931.</li>
<li id="ref-mitchener2025kosmos" data-title="Kosmos: An ai scientist for autonomous discovery" data-short-authors="L. Mitchener et al." data-venue="" data-year="2025">Mitchener, Ludovico; Yiu, Angela; Chang, Benjamin; Bourdenx, Mathieu; Nadolski, Tyler; Sulovari, Arvis; Landsness, Eric C; Barabasi, Daniel L; Narayanan, Siddharth; Evans, Nicky; et al. (2025). Kosmos: An ai scientist for autonomous discovery. arXiv preprint arXiv:2511.02824.</li>
<li id="ref-lu2024discovering" data-title="Discovering preference optimization algorithms with and for large language models" data-short-authors="C. Lu et al." data-venue="Advances in Neural Information Processing Systems" data-year="2024">Lu, Chris; Holt, Samuel; Fanconi, Claudio; Chan, Alex; Foerster, Jakob; van der Schaar, Mihaela; Lange, Robert. (2024). Discovering preference optimization algorithms with and for large language models. Advances in Neural Information Processing Systems, 37, 86528--86573.</li>
<li id="ref-keller1999evolution" data-title="The evolution of genetic code in genetic programming" data-short-authors="R. E. Keller &amp; W. Banzhaf" data-venue="Proceedings of the Genetic and Evolutionary Computation Conference" data-year="1999">Keller, Robert E; Banzhaf, Wolfgang. (1999). The evolution of genetic code in genetic programming. Proceedings of the Genetic and Evolutionary Computation Conference, 2, 1077--1082.</li>
<li id="ref-romera2024mathematical" data-title="Mathematical discoveries from program search with large language models" data-short-authors="B. Romera-Paredes et al." data-venue="Nature" data-year="2024">Romera-Paredes, Bernardino; Barekatain, Mohammadamin; Novikov, Alexander; Balog, Matej; Kumar, M Pawan; Dupont, Emilien; Ruiz, Francisco JR; Ellenberg, Jordan S; Wang, Pengming; Fawzi, Omar; et al. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468--475.</li>
<li id="ref-goldie2025should" data-title="How Should We Meta-Learn Reinforcement Learning Algorithms?" data-short-authors="A. D. Goldie et al." data-venue="" data-year="2025">Goldie, Alexander David; Wang, Zilin; Cohen, Jaron; Foerster, Jakob Nicolaus; Whiteson, Shimon. (2025). How Should We Meta-Learn Reinforcement Learning Algorithms?. arXiv preprint arXiv:2507.17668.</li>
<li id="ref-henderson2018deep" data-title="Deep reinforcement learning that matters" data-short-authors="P. Henderson et al." data-venue="Proceedings of the AAAI conference on artificial intelligence" data-year="2018">Henderson, Peter; Islam, Riashat; Bachman, Philip; Pineau, Joelle; Precup, Doina; Meger, David. (2018). Deep reinforcement learning that matters. Proceedings of the AAAI conference on artificial intelligence, 32.</li>
<li id="ref-engstrom2020implementation" data-title="Implementation matters in deep policy gradients: A case study on ppo and trpo" data-short-authors="L. Engstrom et al." data-venue="" data-year="2020">Engstrom, Logan; Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Janoos, Firdaus; Rudolph, Larry; Madry, Aleksander. (2020). Implementation matters in deep policy gradients: A case study on ppo and trpo. arXiv preprint arXiv:2005.12729.</li>
<li id="ref-huang202237" data-title="The 37 implementation details of proximal policy optimization" data-short-authors="S. Huang et al." data-venue="The ICLR Blog Track 2023" data-year="2022">Huang, Shengyi; Dossa, Rousslan Fernand Julien; Raffin, Antonin; Kanervisto, Anssi; Wang, Weixun. (2022). The 37 implementation details of proximal policy optimization. The ICLR Blog Track 2023.</li>
<li id="ref-adebayo2018sanity" data-title="Sanity checks for saliency maps" data-short-authors="J. Adebayo et al." data-venue="Advances in neural information processing systems" data-year="2018">Adebayo, Julius; Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been. (2018). Sanity checks for saliency maps. Advances in neural information processing systems, 31.</li>
<li id="ref-ferrari2019we" data-title="Are we really making much progress? A worrying analysis of recent neural recommendation approaches" data-short-authors="M. Ferrari Dacrema et al." data-venue="Proceedings of the 13th ACM conference on recommender systems" data-year="2019">Ferrari Dacrema, Maurizio; Cremonesi, Paolo; Jannach, Dietmar. (2019). Are we really making much progress? A worrying analysis of recent neural recommendation approaches. Proceedings of the 13th ACM conference on recommender systems, 101--109.</li>
<li id="ref-chen2021exploring" data-title="Exploring simple siamese representation learning" data-short-authors="X. Chen &amp; K. He" data-venue="Proceedings of the IEEE/CVF conference on computer vision and pattern recognition" data-year="2021">Chen, Xinlei; He, Kaiming. (2021). Exploring simple siamese representation learning. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 15750--15758.</li>
<li id="ref-salimans2017evolution" data-title="Evolution strategies as a scalable alternative to reinforcement learning" data-short-authors="T. Salimans et al." data-venue="" data-year="2017">Salimans, Tim; Ho, Jonathan; Chen, Xi; Sidor, Szymon; Sutskever, Ilya. (2017). Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864.</li>
<li id="ref-mania2018simple" data-title="Simple random search provides a competitive approach to reinforcement learning" data-short-authors="H. Mania et al." data-venue="" data-year="2018">Mania, Horia; Guy, Aurelia; Recht, Benjamin. (2018). Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055.</li>
<li id="ref-gulrajani2020search" data-title="In search of lost domain generalization" data-short-authors="I. Gulrajani &amp; D. Lopez-Paz" data-venue="" data-year="2020">Gulrajani, Ishaan; Lopez-Paz, David. (2020). In search of lost domain generalization. arXiv preprint arXiv:2007.01434.</li>
</ol>